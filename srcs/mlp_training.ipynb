{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410c7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import shlex\n",
    "from colorama import Fore, Style, init\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                              precision_score,\n",
    "                              recall_score,\n",
    "                              f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bfc01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_test_split(X, y, ratio=0.8):\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(X)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(y)\n",
    "    total_rows = X.shape[0]\n",
    "    train_size = int(total_rows * ratio)\n",
    "\n",
    "    # Split data into test and train\n",
    "    X_train = X[:train_size]\n",
    "    X_test = X[train_size:]\n",
    "    y_train = y[:train_size]\n",
    "    y_test = y[train_size:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X = df.drop(columns=[0, 1], axis=1)\n",
    "    X = imputer.fit_transform(X)\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def draw_histogram(file: str) -> None:\n",
    "    df = pd.read_csv(file, header=None)\n",
    "\n",
    "    benign = df[df[1] == 'B']\n",
    "    malignant = df[df[1] == 'M']\n",
    "    columns = df.columns[2:]\n",
    "\n",
    "    feature_list = [\n",
    "        \"Radius\",\n",
    "        \"Texture\",\n",
    "        \"Perimeter\",\n",
    "        \"Area\",\n",
    "        \"Smoothness\",\n",
    "        \"Compactness\",\n",
    "        \"Concavity\",\n",
    "        \"Concave Points\",\n",
    "        \"Symmetry\",\n",
    "        \"Fractal Dimension\",\n",
    "    ]\n",
    "    feat_type = [\"mean\", \"standard error\", \"largest\"]\n",
    "\n",
    "    fig, ax = plt.subplots(6, 5, figsize=(14, 10))\n",
    "    k = 0\n",
    "\n",
    "    for i in range(6):\n",
    "        for j in range(5):\n",
    "            if k < len(columns):\n",
    "                ax[i][j].hist(benign[columns[k]], color='blue', bins=20, alpha=0.4)\n",
    "                ax[i][j].hist(malignant[columns[k]], color='red', bins=20, alpha=0.4)\n",
    "                ax[i][j].set_title(f\"{feature_list[k % 10]} {feat_type[k // 10]}\")\n",
    "            else:\n",
    "                ax[i][j].axis('off')\n",
    "            k += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_graphs(train_loss_Adam, valid_loss_Adam, train_accuracy_Adam, valid_accuracy_Adam,\n",
    "                train_loss_SGD, valid_loss_SGD, train_accuracy_SGD, valid_accuracy_SGD):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax[0].plot(train_loss_Adam, label=\"Train Loss (Adam)\")\n",
    "    ax[0].plot(valid_loss_Adam, label=\"Valid Loss (Adam)\")\n",
    "    ax[0].plot(train_loss_SGD, label=\"Train Loss (SGD)\")\n",
    "    ax[0].plot(valid_loss_SGD, label=\"Valid Loss (SGD)\")\n",
    "    ax[0].set_title(\"Loss Function Convergence\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[0].legend()\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ax[1].plot(train_accuracy_Adam, label=\"Train Accuracy (Adam)\")\n",
    "    ax[1].plot(valid_accuracy_Adam, label=\"Valid Accuracy (Adam)\")\n",
    "    ax[1].plot(train_accuracy_SGD, label=\"Train Accuracy (SGD)\")\n",
    "    ax[1].plot(valid_accuracy_SGD, label=\"Valid Accuracy (SGD)\")\n",
    "    ax[1].set_title(\"Accuracy Function Convergence\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].set_ylabel(\"Accuracy\")\n",
    "    ax[1].legend()\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4de4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_SGD:\n",
    "    def __init__(self, hidden_layer_sizes=(24, 24, 24), learning_rate=0.01,\n",
    "                 n_epochs=1000, batch_size=32, output_size=2):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.no_improve = 0\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.train_loss_history = []\n",
    "        self.valid_loss_history = []\n",
    "        self.train_accuracy_history = []\n",
    "        self.valid_accuracy_history = []\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "    def _binary_cross_entropy(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
    "        loss = -np.mean(y_true * np.log(y_pred) +\n",
    "                        (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "\n",
    "    def _initialize_parameters(self, n_features):\n",
    "        layer_sizes = [n_features] + list(self.hidden_layer_sizes) + [self.output_size]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            fan_in = layer_sizes[i]\n",
    "            fan_out = layer_sizes[i+1]\n",
    "            limit = np.sqrt(6 / (fan_in + fan_out))\n",
    "            self.weights.append(np.random.uniform(-limit, limit, (fan_in, fan_out)))\n",
    "            self.biases.append(np.zeros((1, fan_out)))\n",
    "\n",
    "    def _feedforward(self, X):\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            zs.append(z)\n",
    "            a = self._relu(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        z_output = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(z_output)\n",
    "        y_pred = self._softmax(z_output)\n",
    "        activations.append(y_pred)\n",
    "\n",
    "        return activations, zs\n",
    "\n",
    "    def _backpropagation(self, X_batch, y_batch, activations, zs):\n",
    "\n",
    "        delta = activations[-1] - y_batch\n",
    "\n",
    "        dW = (1/self.batch_size) * np.dot(activations[-2].T, delta)\n",
    "        db = (1/self.batch_size) * np.sum(delta)\n",
    "        self.weights[-1] -= self.learning_rate * dW\n",
    "        self.biases[-1] -= self.learning_rate * db\n",
    "        \n",
    "        for l in range(len(self.weights) - 2, -1, -1):\n",
    "            delta = np.dot(delta, self.weights[l+1].T) * self._relu_derivative(zs[l])\n",
    "            dW = (1/self.batch_size) * np.dot(activations[l].T, delta)\n",
    "            db = (1/self.batch_size) * np.sum(delta)\n",
    "\n",
    "            self.weights[l] -= self.learning_rate * dW\n",
    "            self.biases[l] -= self.learning_rate * db\n",
    "\n",
    "    def fit(self, X, y, X_valid, y_valid):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._initialize_parameters(n_features)\n",
    "\n",
    "        print(\"x_train shape: \", X.shape)\n",
    "        print(\"y_train shape: \", y.shape)\n",
    "        print(\"x_valid shape: \", X_valid.shape)\n",
    "        print(\"y_valid shape: \", y_valid.shape)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # shuffle datasets\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "\n",
    "            # mini-batch loop\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i + self.batch_size]\n",
    "                y_batch = y_shuffled[i:i + self.batch_size]\n",
    "\n",
    "                activations, zs = self._feedforward(X_batch)\n",
    "                self._backpropagation(X_batch, y_batch, activations, zs)\n",
    "\n",
    "            activations, _ = self._feedforward(X)\n",
    "            train_loss = self._binary_cross_entropy(y, activations[-1])\n",
    "            self.train_loss_history.append(train_loss)\n",
    "            y_pred = np.argmax(activations[-1], axis=1)\n",
    "            y_true = np.argmax(y, axis=1)\n",
    "            self.train_accuracy_history.append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "            precision = precision_score(y_true, y_pred)\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "            activations, _ = self._feedforward(X_valid)\n",
    "            valid_loss = self._binary_cross_entropy(y_valid, activations[-1])\n",
    "            self.valid_loss_history.append(valid_loss)\n",
    "            y_pred = np.argmax(activations[-1], axis=1)\n",
    "            y_true = np.argmax(y_valid, axis=1)\n",
    "            self.valid_accuracy_history.append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.n_epochs}, \"\n",
    "                f\"{Fore.YELLOW}Train Loss: {train_loss:.4f}{Style.RESET_ALL}, \"\n",
    "                f\"{Fore.CYAN}Valid Loss: {valid_loss:.4f}{Style.RESET_ALL}, \"\n",
    "                f\"{Fore.MAGENTA}Precision: {precision:.2f}{Style.RESET_ALL}, \"\n",
    "                f\"{Fore.RED}Recall: {recall:.2f}{Style.RESET_ALL}, \"\n",
    "                f\"{Fore.GREEN}F1: {f1:.2f}{Style.RESET_ALL}\")\n",
    "\n",
    "        return self.train_loss_history, self.valid_loss_history, self.train_accuracy_history, self.valid_accuracy_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self._feedforward(X)\n",
    "        return np.argmax(activations[-1], axis=1)\n",
    "\n",
    "    def save_model(self, X_train):\n",
    "        topology = {\n",
    "            'hidden_layer_sizes': self.hidden_layer_sizes,\n",
    "            'input_size': X_train.shape[1],\n",
    "            'output_size': self.output_size,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'n_epochs': self.n_epochs,\n",
    "            'batch_size': self.batch_size,\n",
    "            'activation': 'relu',\n",
    "            'activation_output': 'softmax',\n",
    "        }\n",
    "        np.savez('mlp_weights.npz', *self.weights, *self.biases)\n",
    "        with open('mlp_topology.json', 'w') as f:\n",
    "            json.dump(topology, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0711670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Adam:\n",
    "    def __init__(self, hidden_layer_sizes=(24, 24, 24), learning_rate=0.001,\n",
    "                 n_epochs=1000, batch_size=32, output_size=2, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.no_improve = 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self.m_weights = []\n",
    "        self.v_weights = []\n",
    "        self.m_biases = []\n",
    "        self.v_biases = []\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.valid_loss_history = []\n",
    "        self.train_accuracy_history = []\n",
    "        self.valid_accuracy_history = []\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "    def _binary_cross_entropy(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
    "        loss = -np.mean(y_true * np.log(y_pred) +\n",
    "                        (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "\n",
    "    def _initialize_parameters(self, n_features):\n",
    "        layer_sizes = [n_features] + list(self.hidden_layer_sizes) + [self.output_size]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            fan_in = layer_sizes[i]\n",
    "            fan_out = layer_sizes[i+1]\n",
    "            limit = np.sqrt(6 / (fan_in + fan_out))\n",
    "            self.weights.append(np.random.uniform(-limit, limit, (fan_in, fan_out)))\n",
    "            self.biases.append(np.zeros((1, fan_out)))\n",
    "\n",
    "            self.m_weights.append(np.zeros((fan_in, fan_out)))\n",
    "            self.v_weights.append(np.zeros((fan_in, fan_out)))\n",
    "            self.m_biases.append(np.zeros((1, fan_out)))\n",
    "            self.v_biases.append(np.zeros((1, fan_out)))\n",
    "\n",
    "    def _feedforward(self, X):\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            zs.append(z)\n",
    "            a = self._relu(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        z_output = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(z_output)\n",
    "        y_pred = self._softmax(z_output)\n",
    "        activations.append(y_pred)\n",
    "\n",
    "        return activations, zs\n",
    "\n",
    "    def _backpropagation(self, X_batch, y_batch, activations, zs, t):\n",
    "\n",
    "        delta = activations[-1] - y_batch\n",
    "\n",
    "        grad_w_output = np.dot(activations[-2].T, delta) / X_batch.shape[0]\n",
    "        grad_b_output = np.sum(delta, axis=0) / X_batch.shape[0]\n",
    "\n",
    "        # apply Adam updates to weights\n",
    "        self.m_weights[-1] = self.beta1 * self.m_weights[-1] + (1 - self.beta1) * grad_w_output\n",
    "        self.v_weights[-1] = self.beta2 * self.v_weights[-1] + (1 - self.beta2) * (grad_w_output ** 2)\n",
    "        m_w_hat = self.m_weights[-1] / (1 - self.beta1**t)\n",
    "        v_w_hat = self.v_weights[-1] / (1 - self.beta2**t)\n",
    "        self.weights[-1] -= self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "\n",
    "        # apply Adam updates to bias\n",
    "        self.m_biases[-1] = self.beta1 * self.m_biases[-1] + (1 - self.beta1) * grad_b_output\n",
    "        self.v_biases[-1] = self.beta2 * self.v_biases[-1] + (1 - self.beta2) * (grad_b_output ** 2)\n",
    "        m_b_hat = self.m_biases[-1] / (1 - self.beta1**t)\n",
    "        v_b_hat = self.v_biases[-1] / (1 - self.beta2**t)\n",
    "        self.biases[-1] -= self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "        # Propagate gradients backward through hidden layers\n",
    "        for l in range(len(self.weights) - 2, -1, -1):\n",
    "            delta = np.dot(delta, self.weights[l+1].T) * self._relu_derivative(zs[l]) # d_activation(z)\n",
    "            grad_w_hidden = np.dot(activations[l].T, delta) / X_batch.shape[0]\n",
    "            grad_b_hidden = np.sum(delta, axis=0) / X_batch.shape[0]\n",
    "\n",
    "            # apply Adam updates to weights\n",
    "            self.m_weights[l] = self.beta1 * self.m_weights[l] + (1 - self.beta1) * grad_w_hidden\n",
    "            self.v_weights[l] = self.beta2 * self.v_weights[l] + (1 - self.beta2) * (grad_w_hidden ** 2)\n",
    "            m_w_hat = self.m_weights[l] / (1 - self.beta1**t)\n",
    "            v_w_hat = self.v_weights[l] / (1 - self.beta2**t)\n",
    "            self.weights[l] -= self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "\n",
    "            # apply Adam updates to bias\n",
    "            self.m_biases[l] = self.beta1 * self.m_biases[l] + (1 - self.beta1) * grad_b_hidden\n",
    "            self.v_biases[l] = self.beta2 * self.v_biases[l] + (1 - self.beta2) * (grad_b_hidden ** 2)\n",
    "            m_b_hat = self.m_biases[l] / (1 - self.beta1**t)\n",
    "            v_b_hat = self.v_biases[l] / (1 - self.beta2**t)\n",
    "            self.biases[l] -= self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    def fit(self, X, y, X_valid, y_valid):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._initialize_parameters(n_features)\n",
    "\n",
    "        print(\"x_train shape: \", X.shape)\n",
    "        print(\"y_train shape: \", y.shape)\n",
    "        print(\"x_valid shape: \", X_valid.shape)\n",
    "        print(\"y_valid shape: \", y_valid.shape)\n",
    "\n",
    "        # global time step for Adam bias correction\n",
    "        t = 0\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # shuffle datasets\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "\n",
    "            # mini-batch loop\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i + self.batch_size]\n",
    "                y_batch = y_shuffled[i:i + self.batch_size]\n",
    "                t += 1\n",
    "\n",
    "                activations, zs = self._feedforward(X_batch)\n",
    "                self._backpropagation(X_batch, y_batch, activations, zs, t)\n",
    "\n",
    "            activations, _ = self._feedforward(X)\n",
    "            train_loss = self._binary_cross_entropy(y, activations[-1])\n",
    "            self.train_loss_history.append(train_loss)\n",
    "            y_pred = np.argmax(activations[-1], axis=1)\n",
    "            y_true = np.argmax(y, axis=1)\n",
    "            self.train_accuracy_history.append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "            precision = precision_score(y_true, y_pred)\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "            activations, _ = self._feedforward(X_valid)\n",
    "            valid_loss = self._binary_cross_entropy(y_valid, activations[-1])\n",
    "            self.valid_loss_history.append(valid_loss)\n",
    "            y_pred = np.argmax(activations[-1], axis=1)\n",
    "            y_true = np.argmax(y_valid, axis=1)\n",
    "            self.valid_accuracy_history.append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.n_epochs}, \"\n",
    "                f\"{Fore.YELLOW}Train Loss: {train_loss:.4f}{Style.RESET_ALL}, \"\n",
    "                f\"{Fore.CYAN}Valid Loss: {valid_loss:.4f}{Style.RESET_ALL}, \"\n",
    "                f\"{Fore.MAGENTA}Precision: {precision:.2f}{Style.RESET_ALL}, \"\n",
    "                f\"{Fore.RED}Recall: {recall:.2f}{Style.RESET_ALL}, \"\n",
    "                f\"{Fore.GREEN}F1: {f1:.2f}{Style.RESET_ALL}\")\n",
    "\n",
    "        return self.train_loss_history, self.valid_loss_history, self.train_accuracy_history, self.valid_accuracy_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self._feedforward(X)\n",
    "        return np.argmax(activations[-1], axis=1)\n",
    "\n",
    "    def save_model(self, X_train):\n",
    "        topology = {\n",
    "            'hidden_layer_sizes': self.hidden_layer_sizes,\n",
    "            'input_size': X_train.shape[1],\n",
    "            'output_size': self.output_size,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'n_epochs': self.n_epochs,\n",
    "            'batch_size': self.batch_size,\n",
    "            'activation': 'relu',\n",
    "            'activation_output': 'softmax',\n",
    "        }\n",
    "        np.savez('mlp_weights.npz', *self.weights, *self.biases)\n",
    "        with open('mlp_topology.json', 'w') as f:\n",
    "            json.dump(topology, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(file: str, args: argparse.Namespace):\n",
    "    df = pd.read_csv(file, header=None)\n",
    "\n",
    "    X = preprocess_data(df)\n",
    "    y = df[1].values\n",
    "    y = np.array([[1, 0] if label == 'M' else [0, 1] for label in y])\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = _train_test_split(X, y)\n",
    "\n",
    "    print(\"------------------- Adam ------------------\")\n",
    "    model_Adam = MLP_Adam(\n",
    "        hidden_layer_sizes=args.layer,\n",
    "        learning_rate=args.learning_rate,\n",
    "        n_epochs=args.epochs,\n",
    "        batch_size=args.batch_size\n",
    "    )\n",
    "\n",
    "    train_loss_Adam, valid_loss_Adam, train_accuracy_Adam, valid_accuracy_Adam = model_Adam.fit(X_train, y_train, X_valid, y_valid)\n",
    "    print(\"------------------- SGD ------------------\")\n",
    "\n",
    "    model_SGD = MLP_SGD(\n",
    "        hidden_layer_sizes=args.layer,\n",
    "        learning_rate=args.learning_rate,\n",
    "        n_epochs=args.epochs,\n",
    "        batch_size=args.batch_size\n",
    "    )\n",
    "\n",
    "    train_loss_SGD, valid_loss_SGD, train_accuracy_SGD, valid_accuracy_SGD = model_SGD.fit(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "    plot_graphs(train_loss_Adam, valid_loss_Adam, train_accuracy_Adam, valid_accuracy_Adam,\n",
    "                train_loss_SGD, valid_loss_SGD, train_accuracy_SGD, valid_accuracy_SGD)\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Enter parameters: \")\n",
    "        x = input()\n",
    "        args_list = shlex.split(x)\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--layer', type=int, nargs='+')\n",
    "        parser.add_argument('--epochs', type=int)\n",
    "        parser.add_argument('--batch_size', type=int)\n",
    "        parser.add_argument('--learning_rate', type=float)\n",
    "        args = parser.parse_args(args_list)\n",
    "\n",
    "        train_model(\"../dataset/data.csv\", args)\n",
    "    except AssertionError as error:\n",
    "        print(AssertionError.__name__ + \":\", error)\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
