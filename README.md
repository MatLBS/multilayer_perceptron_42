# üá¨üáß multilayer_perceptron_42

**If you enjoy this project, feel free to give it a star ‚≠êÔ∏è!**

## Introduction

The goal of this project is to build a multi-layer perceptron (MLP) from scratch to create a model that predicts the outcome of a breast cancer diagnosis.
---

## Multilayer perceptron
The multilayer perceptron is a feedforward network (meaning that the data
flows from the input layer to the output layer) defined by the presence of one or more
hidden layers, as well as an interconnection of all the neurons of one layer to the next.

<img width="697" height="431" alt="Screenshot from 2025-09-25 15-44-50" src="https://github.com/user-attachments/assets/ee5f8d55-e87e-4cd5-a206-e1589700a127" />

The diagram above represents a network containing 4 dense layers (also called fully
connected layers). Its inputs consist of 4 neurons and its output consists of 2 (perfect
for binary classification). The weights of one layer to the next are represented by
two-dimensional matrices noted Wlj lj+1 . The matrix Wl0l1 is of size (3, 4) for example, as
it contains the weights of the connections between layer l0 and layer l1.
The bias is often represented as a special neuron which has no inputs and an output
always equal to 1. Like a perceptron, it is connected to all the neurons of the following
layer (the bias neurons are noted blj on the diagram above). The bias is generally useful
as it allows one to ‚Äúcontrol the behavior‚Äù of a layer.

## Perceptron
The perceptron is the type of neuron that the multilayer perceptron is composed
of. It is defined by the presence of one or more input connections, an activation
function, and a single output. Each connection contains a weight (also called a
parameter) which is learned during the training phase.

<img width="595" height="266" alt="Screenshot from 2025-09-25 15-46-18" src="https://github.com/user-attachments/assets/c4947a3f-ac26-4bca-962f-dc2eb0a1bebc" />

Two steps are necessary to get the output of a neuron. The first one consists in
computing the weighted sum of the outputs of the previous layer with the weights of the
input connections of the neuron, which gives

<img width="323" height="46" alt="Screenshot from 2025-09-25 15-47-18" src="https://github.com/user-attachments/assets/6f236407-0511-44bc-8a65-0e2bcdca3599" />

The second step consists in applying an activation function on this weighted sum.
The output of this function is the output of the perceptron and can be understood as
the threshold above which the neuron is activated (activation functions can take many
forms, and you are free to choose whichever one you want depending on the model to
train. Here are some of the most frequently used ones to give you an idea: sigmoid,
hyperbolic tangent, and rectified linear unit).

## Usage

1. Clone the repository
```bash
git clone https://github.com/MatLBS/ft_linear_regression_42.git
cd ft_linear_regression_42
```
2. Create a virtual environment and install dependencies
```python
python -m venv myenv
source myenv/bin/activate
pip install -r requirements.txt
```

## Dataset preview

```bash
python srcs/describe.py dataset/data.csv
```

<img width="1010" height="535" alt="Screenshot from 2025-09-25 15-53-31" src="https://github.com/user-attachments/assets/884e4570-3c56-47b8-901f-2496bab916ef" />

```bash
python srcs/histogram.py dataset/data.csv
```

<img width="1420" height="1097" alt="Screenshot from 2025-09-25 15-54-11" src="https://github.com/user-attachments/assets/2a5a0f35-0a2c-4f57-98ee-bb93013937ea" />

---

# Training & Prediction

## Training of an MLP model

```bash
python srcs/mlp_training.py --file dataset/data.csv --layer 24 24 --epochs 2000 --batch_size 32 --learning_rate 0.001
```

<img width="689" height="559" alt="Screenshot from 2025-09-25 15-57-08" src="https://github.com/user-attachments/assets/97223140-3389-44db-8bd8-1aa64586cf8f" />

<img width="1220" height="597" alt="Screenshot from 2025-09-25 15-56-34" src="https://github.com/user-attachments/assets/abb43e4e-8184-40ed-bb19-93fbc70953dc" />

## Prediction and evaluation
You can predict the test data and evaluate your model by running below command.
```bash
python srcs/mlp_predict.py mlp_weights.npz mlp_topology.json dataset/data.csv
```

<img width="258" height="18" alt="Screenshot from 2025-09-25 15-59-10" src="https://github.com/user-attachments/assets/b6308c2c-1e2b-46d7-af40-34e26672cd5f" />

# üá´üá∑ multilayer_perceptron_42

**Si tu appr√©cies ce projet, n‚Äôh√©site pas √† lui donner une √©toile ‚≠êÔ∏è !**

## Introduction

L‚Äôobjectif de ce projet est de construire un perceptron multicouche (MLP) ‚Äúfrom scratch‚Äù pour cr√©er un mod√®le capable de pr√©dire l‚Äôissue d‚Äôun diagnostic du cancer du sein.

---

## Perceptron multicouche (MLP)

Le perceptron multicouche est un r√©seau ‚Äúfeedforward‚Äù (ce qui signifie que les donn√©es circulent de la couche d‚Äôentr√©e √† la couche de sortie) d√©fini par la pr√©sence d‚Äôune ou plusieurs couches cach√©es, ainsi que par l‚Äôinterconnexion de tous les neurones d‚Äôune couche √† la suivante.

<img width="697" height="431" alt="Screenshot from 2025-09-25 15-44-50" src="https://github.com/user-attachments/assets/ee5f8d55-e87e-4cd5-a206-e1589700a127" />

Le sch√©ma ci-dessus repr√©sente un r√©seau contenant 4 couches denses (ou ‚Äúfully connected‚Äù). Ses entr√©es sont constitu√©es de 4 neurones et sa sortie de 2 (parfait pour une classification binaire). Les poids d‚Äôune couche √† l‚Äôautre sont repr√©sent√©s par des matrices not√©es Wlj lj+1. Par exemple, la matrice Wl0l1 est de taille (3, 4), car elle contient les poids des connexions entre la couche l0 et la couche l1.

Le biais est souvent repr√©sent√© comme un neurone sp√©cial qui n‚Äôa pas d‚Äôentr√©e et une sortie toujours √©gale √† 1. Comme un perceptron, il est connect√© √† tous les neurones de la couche suivante (les neurones de biais sont not√©s blj sur le sch√©ma ci-dessus). Le biais est g√©n√©ralement utile car il permet de ‚Äúcontr√¥ler le comportement‚Äù d‚Äôune couche.

## Perceptron

Le perceptron est le type de neurone qui compose le perceptron multicouche. Il est d√©fini par la pr√©sence d‚Äôune ou plusieurs connexions d‚Äôentr√©e, d‚Äôune fonction d‚Äôactivation et d‚Äôune seule sortie. Chaque connexion poss√®de un poids (aussi appel√© param√®tre) qui est appris lors de la phase d‚Äôentra√Ænement.

<img width="595" height="266" alt="Screenshot from 2025-09-25 15-46-18" src="https://github.com/user-attachments/assets/c4947a3f-ac26-4bca-962f-dc2eb0a1bebc" />

Deux √©tapes sont n√©cessaires pour obtenir la sortie d‚Äôun neurone. La premi√®re consiste √† calculer la somme pond√©r√©e des sorties de la couche pr√©c√©dente avec les poids des connexions d‚Äôentr√©e du neurone, ce qui donne :

<img width="323" height="46" alt="Screenshot from 2025-09-25 15-47-18" src="https://github.com/user-attachments/assets/6f236407-0511-44bc-8a65-0e2bcdca3599" />

La seconde √©tape consiste √† appliquer une fonction d‚Äôactivation sur cette somme pond√©r√©e. La sortie de cette fonction est la sortie du perceptron et peut √™tre comprise comme le seuil au-del√† duquel le neurone s‚Äôactive (les fonctions d‚Äôactivation peuvent prendre diff√©rentes formes, et tu es libre de choisir celle qui te convient selon le mod√®le √† entra√Æner. Voici quelques-unes des plus utilis√©es : sigmo√Øde, tangente hyperbolique, et rectified linear unit).

## Utilisation

1. Clone le d√©p√¥t
```bash
git clone https://github.com/MatLBS/ft_linear_regression_42.git
cd ft_linear_regression_42
```
2. Cr√©e un environnement virtuel et installe les d√©pendances
```python
python -m venv myenv
source myenv/bin/activate
pip install -r requirements.txt
```

## Aper√ßu du dataset

```bash
python srcs/describe.py dataset/data.csv
```

<img width="1010" height="535" alt="Screenshot from 2025-09-25 15-53-31" src="https://github.com/user-attachments/assets/884e4570-3c56-47b8-901f-2496bab916ef" />

```bash
python srcs/histogram.py dataset/data.csv
```

<img width="1420" height="1097" alt="Screenshot from 2025-09-25 15-54-11" src="https://github.com/user-attachments/assets/2a5a0f35-0a2c-4f57-98ee-bb93013937ea" />

---

# Entra√Ænement & Pr√©diction

## Entra√Ænement d‚Äôun mod√®le MLP

```bash
python srcs/mlp_training.py --file dataset/data.csv --layer 24 24 --epochs 2000 --batch_size 32 --learning_rate 0.001
```

<img width="689" height="559" alt="Screenshot from 2025-09-25 15-57-08" src="https://github.com/user-attachments/assets/97223140-3389-44db-8bd8-1aa64586cf8f" />

<img width="1220" height="597" alt="Screenshot from 2025-09-25 15-56-34" src="https://github.com/user-attachments/assets/abb43e4e-8184-40ed-bb19-93fbc70953dc" />

## Pr√©diction et √©valuation

Tu peux pr√©dire les donn√©es de test et √©valuer ton mod√®le en lan√ßant la commande suivante :
```bash
python srcs/mlp_predict.py mlp_weights.npz mlp_topology.json dataset/data.csv
```

<img width="258" height="18" alt="Screenshot from 2025-09-25 15-59-10" src="https://github.com/user-attachments/assets/b6308c2c-1e2b-46d7-af40-34e26672cd5f" />
